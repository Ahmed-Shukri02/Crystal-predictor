{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f6ade84",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import math\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.neighbors import KNeighborsClassifier as knn\n",
    "from sklearn.ensemble import RandomForestClassifier as randForest\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "class Preprocessor():\n",
    "    def __init__(self, file_directory: str, drop_list: list, not_number: list, filetype: str = \"csv\", \n",
    "                 encodetype: str = \"label\", useScaler: bool = False, scaletype: str = \"minmax\", \n",
    "                 replaceNAN: bool = True):\n",
    "        \"\"\"Init function\n",
    "            Inputs: \n",
    "            file_directory: directory of the file relative to script.\n",
    "            filetype: type of file to be imported (defaulted as csv).\n",
    "            drop_list: list of columns to omit from the preprocessed data.\n",
    "            not_number: list of columns that are worded for encoded (EXCEPT last column)\n",
    "            encodetype: type of encoder to be used (ONLY onehot or label). Defaulted to label\n",
    "            useScaler: boolean that states whether the data should be scaled or not\n",
    "            scaletype: type of scaler to be used if useScalercaler is true (ONLY minmax and standard)\n",
    "            replaceNAN: true if all N/A values should be replaced by the mean of the column, false if row\n",
    "            containing N/A should be omitted (defaulted to true).\n",
    "            \n",
    "            Returns: None\"\"\"\n",
    "        \n",
    "        # add attributes\n",
    "        self.dir = file_directory\n",
    "        self.worded = not_number\n",
    "        self.drop_list = drop_list\n",
    "        self.filetype = filetype\n",
    "        self.scale = useScaler\n",
    "        self.replaceNAN = replaceNAN\n",
    "        \n",
    "        # make sure encodetype is either onehot or label\n",
    "        encode_check = encodetype in (\"onehot\", \"label\")\n",
    "        assert encode_check, f\"Unexpected encode type '{encodetype}'. Preprocessor only takes onehot or label encoding\"\n",
    "        # add encoder attribute\n",
    "        self.encodetype = encodetype\n",
    "        \n",
    "        # only run if useScaler is true, else scaletype won't be defined.\n",
    "        if(useScaler == True):\n",
    "            # make sure scaletype is either minmax or standard\n",
    "            scale_check = scaletype in (\"minmax\", \"standard\")\n",
    "            assert scale_check, f\"Unexpected scaling type '{scaletype}'. Preprocessor only takes minmax or standard scaling\"\n",
    "            # add scaler attribute\n",
    "            self.scaletype = scaletype\n",
    "        \n",
    "    \n",
    "\n",
    "    def Process(self) -> pd.DataFrame:\n",
    "        \"\"\"Calls all other functions within class and runs until a \n",
    "            clean, encoded, scaled Dataframe is returned to the user\n",
    "            Inputs: \n",
    "            self: contains attributes (refer to notes at __init__)\n",
    "            \n",
    "            Returns: pandas dataframe of fully preprocessed data\"\"\"\n",
    "        \n",
    "        # first we import the file:\n",
    "        raw_data = self.ImportAsDf()\n",
    "        # we then clean the data:\n",
    "        clean_data, worded_columns = self.CleanData(raw_data)\n",
    "        # next we encode the necessary worded columns:\n",
    "        encoded_data = self.Encode(clean_data, worded_columns)\n",
    "        \n",
    "        # if useScaler is true, scale the non-encoded numerated data:\n",
    "        if(self.scale == True):\n",
    "            # get list of non-encoded numerated columns\n",
    "            numbered_columns = [i for i in encoded_data.columns if i not in worded_columns]\n",
    "            \n",
    "            # scale data\n",
    "            scaled = self.Scale(encoded_data, numbered_columns)\n",
    "            return scaled\n",
    "        \n",
    "        else: return encoded_data\n",
    "    \n",
    "    \n",
    "    def ImportAsDf(self) -> pd.DataFrame:\n",
    "        \"\"\"Grabs file and imports it as a pandas Dataframe.\n",
    "            Inputs: \n",
    "            self: contains attributes (refer to notes at __init__).\n",
    "\n",
    "            Returns: pandas dataframe of the raw data\"\"\"\n",
    "\n",
    "        if(self.filetype == \"csv\"):\n",
    "            return pd.read_csv(self.dir)\n",
    "        return None\n",
    "\n",
    "    \n",
    "    def CleanData(self, data: pd.DataFrame) -> tuple:\n",
    "        \"\"\"Goes through the data and removes/replaces any N/A values, as well\n",
    "           returning any worded columns to to be encoded.\n",
    "            Inputs: \n",
    "            self: contains attributes (refer to notes at __init__).\n",
    "            data: data to be cleaned. \n",
    "\n",
    "            Returns: tuple: \n",
    "                   0: pandas dataframe of the cleaned data\n",
    "                   1: list of all worded columns to be encoded\"\"\"\n",
    "\n",
    "        #reset index of data \n",
    "        data = data.reset_index(drop=True)\n",
    "        #remove unnecessary columns\n",
    "        data = data.drop(self.drop_list, axis=1)\n",
    "        #replace all \"-\" with NAN\n",
    "        data = data.replace(\"-\", np.nan)\n",
    "\n",
    "        #get two lists of columns split on whether their values are numbers or not:\n",
    "        worded_columns = [i for i in self.worded if i in data] # makes sure worded columns that were dropped do not appear\n",
    "        numerated_columns = [i for i in data.columns if i not in worded_columns]\n",
    "\n",
    "        # we first search through worded columns and remove row where NA is present, since mean method will not work:\n",
    "        for column in worded_columns:\n",
    "            removal_list=[]\n",
    "            \n",
    "            # go through every value in worded column\n",
    "            for i, value in enumerate(data[column]):\n",
    "                \n",
    "                # add row index of each occurance of NAN\n",
    "                if(pd.isna(value)): removal_list.append(i)\n",
    "            \n",
    "            # remove once iteration is complete to avoid issues with index i:\n",
    "            data.drop(removal_list, inplace=True)\n",
    "\n",
    "        \n",
    "        # we then search through numbered columns, and either replace NA with a mean or remove column    \n",
    "        if(self.replaceNAN== True):\n",
    "            for column in numerated_columns:\n",
    "                #find mean of column (convert all values in columns to floats before averaging)\n",
    "                column_mean = data[column].astype(float).mean(skipna= True)\n",
    "                \n",
    "                #replace na values on column with mean\n",
    "                data[column] = data[column].apply(lambda x: column_mean if pd.isna(x) == True else x)\n",
    "            \n",
    "            return (data.reset_index(drop=True), worded_columns)\n",
    "        \n",
    "        \n",
    "        else: # remove NA values if repalce is set to false:\n",
    "            return (data.dropna().reset_index(drop=True), worded_columns)\n",
    "\n",
    "    \n",
    "    def Encode(self, data: pd.DataFrame, encode_columns: list) -> pd.DataFrame:\n",
    "        \"\"\"Encodes columns either using a label encoder or a onehot encoder.\n",
    "            Inputs:\n",
    "            self: contains attributes (refer to notes at __init__)\n",
    "            data: data to be encoded.\n",
    "            encode_columns: list of columns by label to be encoded\n",
    "            \n",
    "            Returns:\n",
    "            Pandas dataframe of the encoded data\"\"\"\n",
    "        \n",
    "        # exclude final column from being encoded since it is the dependant variable\n",
    "        encode_columns = encode_columns[:-1]\n",
    "        \n",
    "        #check if onehot or label encoder is being used\n",
    "        if(self.encodetype == \"onehot\"):\n",
    "            for column in encode_columns:\n",
    "                # grab a list of distinct categories\n",
    "                categories = sorted(set(data[column]))\n",
    "                \n",
    "                # go through each category and make a new column with binary values 0 or 1\n",
    "                for category in categories:\n",
    "                    data[f\"{column}_{category}\"] = data[column].apply(lambda x: 1 if x == category else 0)\n",
    "                \n",
    "                #drop column\n",
    "                data.drop(column, axis=1, inplace = True)\n",
    "                \n",
    "            return data\n",
    "        \n",
    "        else: # must be label due to previous assertion\n",
    "            for column in encode_columns:\n",
    "                # grab a list of distinct categories\n",
    "                categories = sorted(set(data[column]))\n",
    "                \n",
    "                # map these categories to integers:\n",
    "                category_map = {categories[i] : i for i in range(len(categories))}\n",
    "                \n",
    "                # apply value using the map\n",
    "                data[column] = data[column].apply(lambda x: category_map[x])\n",
    "            \n",
    "            return data\n",
    "    \n",
    "    \n",
    "    def Scale(self, data: pd.DataFrame, scale_columns: list) -> pd.DataFrame:\n",
    "        \"\"\"Scales the data based on the minmax or standard approach\n",
    "            Inputs:\n",
    "            self: contains attributes (refer to notes at __init__)\n",
    "            data: data to be scaled.\n",
    "            scale_columns: list of columns by label to be scaled\n",
    "            \n",
    "            Returns:\n",
    "            Pandas dataframe of the scaled data\"\"\"\n",
    "        \n",
    "        # check if minmax or standard scaler is being used\n",
    "        if (self.scaletype == \"minmax\"):\n",
    "            # for each column of the data find the minimum and maximum values:\n",
    "            for column in scale_columns:\n",
    "                # make all values in column a float\n",
    "                data[column] = data[column].astype(float)\n",
    "                \n",
    "                min_value, max_value = (data[column].min(), data[column].max())\n",
    "                column_range = max_value - min_value\n",
    "                \n",
    "                # change data using minmax equation:\n",
    "                data[column] = data[column].apply(lambda x: (x - min_value)/column_range)\n",
    "            \n",
    "            return data\n",
    "        \n",
    "        else: # must be standard due to previous assertion\n",
    "            # for each column of the data, find the mean and standard deviation\n",
    "            for column in scale_columns:\n",
    "                # make all values in column a float\n",
    "                data[column] = data[column].astype(float)\n",
    "                \n",
    "                mean, std = (data[column].mean(), data[column].std())\n",
    "                \n",
    "                # change data using standard equation:\n",
    "                data[column] = data[column].apply(lambda x: (x - mean)/std)\n",
    "            \n",
    "            return data\n",
    "\n",
    "\n",
    "# create list of worded columns and list of columns to drop\n",
    "classes = [\"Compound\", \"A\", \"B\", \"In literature\", \"Lowest distortion\"]\n",
    "drop_list = [\"Compound\", \"In literature\", \"A\", \"B\", \"Ï„\"]\n",
    "\n",
    "file_directory = \"Crystal_structure.csv\"\n",
    "\n",
    "# get preprocessed data\n",
    "clean = Preprocessor(file_directory, drop_list, classes, replaceNAN = True, \n",
    "                     useScaler = True, scaletype = \"standard\").Process()\n",
    "\n",
    "\n",
    "\n",
    "def SplitData(data: pd.DataFrame, train_ratio: float = 0.5) -> tuple:\n",
    "    \"\"\"Splits data into train and test samples\n",
    "        Inputs:\n",
    "        data: data to be split\n",
    "        train_ratio: the ratio of train to test array size for the data.\n",
    "        (defaulted to 10% train with 90% test data)\n",
    "        \n",
    "        Returns: tuple:\n",
    "                 0: pandas DataFrame of train data for independent variables.\n",
    "                 1: pandas DataFrame of test data for independent variables.\n",
    "                 2: pandas Series of train data for dependent variable.\n",
    "                 3: pandas Series of test data for dependent variable.\"\"\"\n",
    "    \n",
    "    # find index where data should be split\n",
    "    index = int(train_ratio * len(data.index))\n",
    "    \n",
    "    # Make a list of dependent and independant variables:\n",
    "    X_columns = list(data.columns)\n",
    "    y_columns = X_columns.pop(-1)\n",
    "    \n",
    "    # split X and y data into test and training data for each\n",
    "    train = data.sample(frac=train_ratio, random_state = 200)\n",
    "    test = data.drop(train.index)\n",
    "    \n",
    "    return train[X_columns], test[X_columns], train[y_columns], test[y_columns]\n",
    "    \n",
    "\n",
    "\n",
    "# split data\n",
    "split_data = SplitData(clean)\n",
    "# split data will be used for classification testing and cross-validation, check CLassification.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a25cf4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
