{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "07be691b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def KNN(data: tuple, show_graph: bool = True) -> tuple: \n",
    "    \"\"\"post-test note: KNN regression for this exact data is slightly unreliable, since the data is imbalanced.\n",
    "       \n",
    "       K nearest neighbours algorithm that classifies the test data and returns the classifier with the best\n",
    "       value of K\n",
    "        Inputs:\n",
    "        data: tuple of train and test data for both dependent and independent variables\n",
    "        \n",
    "        Returns:\n",
    "        tuple of KNeighborsClassifier object of the classifier with the highest f1 score and the metrics report in\n",
    "        the format of a dictionary\"\"\"\n",
    "    \n",
    "    # unpack the split data\n",
    "    X_train, X_test, y_train, y_test = data\n",
    "    \n",
    "    \n",
    "    # find an appropriate number to use for k\n",
    "    # go through knn algorithm for different k to find the best value of k between 1 and a limit\n",
    "    limit = int(math.sqrt(len(y_test)))\n",
    "    \n",
    "    regressor_dict = {}\n",
    "    \n",
    "    for k in range(limit): # we must only use odd numbers so 2k+1 will be used as index\n",
    "        # initate knn and fit classifier to the data:\n",
    "        classifier = knn(n_neighbors = (2*k)+1, p=2, metric = \"euclidean\")\n",
    "        classifier.fit(X_train, y_train)\n",
    "        \n",
    "        # predict y for test data and compare to real y data:\n",
    "        y_pred = classifier.predict(X_test)\n",
    "        score = metrics.f1_score(y_test, y_pred, average = \"weighted\")\n",
    "        \n",
    "        # add classifier with their score into dictionary\n",
    "        regressor_dict[classifier] =  score\n",
    "\n",
    "    # return the key with the highest value:\n",
    "    best_classifier = max(regressor_dict, key=regressor_dict.get)\n",
    "    best_k = best_classifier.n_neighbors\n",
    "    \n",
    "    # re-predict y and collect a general metric report in dictionary format\n",
    "    best_y_pred = best_classifier.predict(X_test)\n",
    "    report = metrics.classification_report(y_test, best_y_pred, output_dict = True)\n",
    "    \n",
    "    if(show_graph == True):\n",
    "        print(f\"\\n>>>>>\\n KNN Algorithm \\n Best value of k for KNN is: {best_classifier.n_neighbors} \\n\")\n",
    "        \n",
    "        # print general metric report in string format\n",
    "        print(metrics.classification_report(y_test, best_y_pred))\n",
    "\n",
    "        # Visualise the deviation of predicted data from real data\n",
    "        VisualisePrediction(y_test, best_y_pred, f\"K Nearest Neighbours: K = {best_k}\" ,alpha_min = 0.2)\n",
    "    \n",
    "        print(f\"\\n<<<<<\\n\")\n",
    "    \n",
    "    return best_classifier, report\n",
    "    \n",
    "\n",
    "\n",
    "def Logistic(data: tuple, show_graph: bool = True) -> tuple:\n",
    "    \"\"\"post-test note: Performes slightly worse than KNN, with the lowest f1 score\n",
    "       \n",
    "       Multinomial logsitic regression that classifies the test data and returns the classifier\n",
    "        Inputs:\n",
    "        data: tuple of train and test data for both dependent and independent variables\n",
    "        \n",
    "        Returns:\n",
    "        tuple of LogisticRegression object of the classifier and the metrics report in the \n",
    "        format of a dictionary\"\"\"\n",
    "    \n",
    "    # unpack the split data\n",
    "    X_train, X_test, y_train, y_test = data\n",
    "\n",
    "    # initiallise a multinomial logistic regression and fit to training set\n",
    "    classifier = LogisticRegression(solver='lbfgs',class_weight='balanced', max_iter=10000) \n",
    "    classifier.fit(X_train, y_train)\n",
    "\n",
    "    # calculate prediction and collect a general metric report in dictionary format\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    report = metrics.classification_report(y_test, y_pred, output_dict = True)\n",
    "    \n",
    "    if(show_graph == True):\n",
    "        print(\"\\n>>>>>\\n Multinomial Logistic Regression report \\n\")\n",
    "        \n",
    "        # print general metric report in string format\n",
    "        print(metrics.classification_report(y_test, y_pred))\n",
    "\n",
    "        # Visualise the deviation of predicted data from real data\n",
    "        VisualisePrediction(y_test, y_pred, \"Multinomial Logistic Regression\", alpha_min = 0.2)\n",
    "\n",
    "        print(f\"\\n<<<<<\\n\")\n",
    "    \n",
    "    return classifier, report\n",
    "    \n",
    "\n",
    "def RandomForest(data: tuple, n_limit: int = 50, show_graph: bool = True) -> tuple:\n",
    "    \"\"\"post-test note: Performes the best out of all classifiers, with a good f1 score\n",
    "       \n",
    "       Random Forest algorithm that averages betweeen n_estimators decision trees.\n",
    "        Inputs:\n",
    "        data: tuple of train and test data for both dependent and independent variables\n",
    "        n_limit: limit to the number of decision trees tested for the classification\n",
    "        \n",
    "        Returns:\n",
    "        tuple of RandomForestClassifier object of the classifier and the metrics report\n",
    "        in the format of a dictionary\"\"\"\n",
    "    \n",
    "    # unpack the split data\n",
    "    X_train, X_test, y_train, y_test = data\n",
    "    \n",
    "    # Random Forest requires y values to be encoded, so we will encode it.\n",
    "    # recombine y split\n",
    "    y_combined = pd.concat([y_train, y_test], axis = 0)\n",
    "    \n",
    "    # use quick label encoding for combined y\n",
    "    categories = sorted(set(y_combined))         \n",
    "    category_map = {categories[i] : i for i in range(len(categories))}        \n",
    "    y_combined = y_combined.apply(lambda x: category_map[x])\n",
    "    \n",
    "    # resplit y into train and test\n",
    "    y_train = y_combined[:len(y_train)]\n",
    "    y_test = y_combined[len(y_train):]\n",
    "    \n",
    "    regressor_dict = {}\n",
    "    \n",
    "    # find an appropriate number to use for n_estimators\n",
    "    for n_estimators in range(n_limit):\n",
    "        # for each n_estimators, classify data\n",
    "        classifier = randForest(n_estimators = n_estimators + 1, criterion = 'entropy', random_state = 200)\n",
    "        classifier.fit(X_train, y_train)\n",
    "\n",
    "        # calculate prediction and get score\n",
    "        y_pred = classifier.predict(X_test)\n",
    "        score = metrics.f1_score(y_test, y_pred, average = \"weighted\")\n",
    "        \n",
    "        # add classifier with their score into dictionary\n",
    "        regressor_dict[classifier] =  score\n",
    "\n",
    "    # return the key with the highest value:\n",
    "    best_classifier = max(regressor_dict, key=regressor_dict.get)\n",
    "    best_n = best_classifier.n_estimators\n",
    "    \n",
    "    # re-predict y and collect a general metric report in dictionary format\n",
    "    best_y_pred = best_classifier.predict(X_test)\n",
    "    report = metrics.classification_report(y_test, best_y_pred, output_dict = True)\n",
    "    \n",
    "    if(show_graph == True):\n",
    "        print(f\"\\n>>>>>\\nRandom Forest Algorithm \\n Best value of n for Random Forest is: \\\n",
    "              {best_classifier.n_estimators} \\n\")\n",
    "        \n",
    "        # print general metric report in string format\n",
    "        print(metrics.classification_report(y_test, best_y_pred))\n",
    "\n",
    "        # Visualise the deviation of predicted data from real data\n",
    "        VisualisePrediction(y_test, best_y_pred, f\"Random Forest Classifier: \"\n",
    "                            f\"{best_n} Decision Trees\", alpha_min = 0.2)\n",
    "    \n",
    "        print(f\"\\n<<<<<\\n\")\n",
    "    \n",
    "    return best_classifier, report\n",
    "\n",
    "    \n",
    "def VisualisePrediction(y_test: pd.Series, y_pred: pd.Series, classifier_name: str, alpha_min: float = 0.5,) -> None:\n",
    "    \"\"\"Visualises prediction of classification algorithm by comparing the real value of the\n",
    "       dependant variable with the one predicted by the algorithm and plotting the deviation\n",
    "       of one from the other. Plots the points as a density map, with higher number of points \n",
    "       for a coordinate being darker and larger.\n",
    "        Inputs: \n",
    "        y_test: pandas series of the real values of the dependent variable.\n",
    "        y_pred: pandas series of the predicted values of dependent variable.\n",
    "        classifier_name: the name of classifier used to generate y_pred. Used as the title of the plot.\n",
    "        alpha_min: float of the minimum alpha value for the density plot.\n",
    "        \n",
    "        Returns: None\"\"\"\n",
    "    \n",
    "    # create dataframe of y_test and y_pred\n",
    "    data = pd.DataFrame({\"test\": y_test, \"pred\": y_pred})\n",
    "    \n",
    "    count_dict = {}\n",
    "    # go through each category present in test data:\n",
    "    for test_category in set(data[\"test\"]):\n",
    "        # go through each category in predicted data:\n",
    "        for pred_category in set(data[\"pred\"]):\n",
    "            # get a dataframe with only occurances of a specific coordinate, \n",
    "            # and find the number of occurances of this coordinate\n",
    "            coordinate_data = data[(data[\"test\"] == test_category) & (data[\"pred\"] == pred_category)]\n",
    "            no_occurances = len(coordinate_data)\n",
    "            \n",
    "            # add coordinate with their count into a dictionary\n",
    "            count_dict[(test_category, pred_category)] = no_occurances\n",
    "        \n",
    "    \n",
    "    # find coordinates with maximum and minimum counts\n",
    "    max_key, min_key = (max(count_dict, key = count_dict.get), min(count_dict, key = count_dict.get))\n",
    "    max_count, min_count = (count_dict[max_key], count_dict[min_key])\n",
    "    \n",
    "    # create new dataframe of unique rows, these will be our coordinates\n",
    "    set_data = data.drop_duplicates(inplace = False)\n",
    "    \n",
    "    # set a quick min-max scaler that will scale all the counts to an alpha range\n",
    "    for key in count_dict.keys():\n",
    "        # first grab the value of the key\n",
    "        count = count_dict[key]\n",
    "        \n",
    "        # then we apply a special min-max scaler using max of 1 and min of alpha_lower\n",
    "        count_range = max_count - min_count\n",
    "        alpha_range = 1 - alpha_min\n",
    "        \n",
    "        min_diff = count - min_count\n",
    "        \n",
    "        # compute scaled value and replace in dictionary\n",
    "        alpha = (min_diff)/(count_range) * alpha_range + alpha_min\n",
    "        count_dict[key] = alpha\n",
    "        \n",
    "    \n",
    "    # create figure and axes\n",
    "    fig, ax = plt.subplots(figsize = (9,9))\n",
    "    \n",
    "    # plot the y_test reference values into the figure\n",
    "    ax.plot(y_test, y_test, \"r-\", label = \"Test data\")\n",
    "    \n",
    "    # plot each coordinate of the y_test - y_pred plot individually with alpha set:\n",
    "    for coordinates, alpha in count_dict.items():\n",
    "        test, pred = (coordinates[0], coordinates[1])\n",
    "        ax.plot(test, pred, \"ko\", alpha = alpha, markersize = 20*alpha)\n",
    "    \n",
    "    # set title and labels\n",
    "    ax.set_title(classifier_name, fontsize = 15)\n",
    "    ax.set_ylabel(\"Predicted value\")\n",
    "    ax.set_xlabel(\"Test data\")\n",
    "    \n",
    "    # create empty line for legend\n",
    "    ax.plot(0, 0, \"ko\", label = \"Predicted values\")\n",
    "    \n",
    "    # show legend and figure\n",
    "    ax.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def CrossValidate(split_data: tuple, classifiers: dict) -> dict:\n",
    "    \"\"\"Runs several classifiers for k different folds. f1 scores for average, weighted\n",
    "       and accuracy are averaged through all folds, returning a single of each score for\n",
    "       each classifier\n",
    "        Inputs:\n",
    "        split_data: tuple containing k different train-test tuples\n",
    "        classifiers: tuple containing different classifiers to be used \n",
    "        \n",
    "        Returns: dictionary of name of classifier to their f1 scores stored in a tuple, where:\n",
    "                0: accuracy score, 1: macro average score, 2: weighted average score.\"\"\"\n",
    "    \n",
    "    classifier_scores = {}\n",
    "\n",
    "    # find the average \n",
    "    for classifier in classifiers:\n",
    "        \n",
    "        # make empty lists of different scores to append for each fold\n",
    "        classifier_f1_macro = []\n",
    "        classifier_f1_weighted = []\n",
    "        classifier_f1_accuracy = []\n",
    "\n",
    "        # unpack fold into index and train-test tuple\n",
    "        for i, fold in enumerate(split_data):\n",
    "            # run classifier algorithm and return score measures\n",
    "            # print(f\"\\nFOLD {i + 1}\\n\")\n",
    "            classifier_obj, report_dict = classifier(fold, show_graph = False)\n",
    "            \n",
    "            # find accuracy score and add it into list\n",
    "            classifier_f1_accuracy.append(report_dict[\"accuracy\"])\n",
    "\n",
    "            # find macro and weighted scores\n",
    "            macro_scores = report_dict[\"macro avg\"]\n",
    "            weighted_scores = report_dict[\"weighted avg\"]\n",
    "\n",
    "            # from these scores, add f1 score to list\n",
    "            classifier_f1_macro.append(macro_scores[\"f1-score\"])\n",
    "            classifier_f1_weighted.append(weighted_scores[\"f1-score\"])\n",
    "\n",
    "        # average the scores for all categories\n",
    "        classifier_accuracyAverage = sum(classifier_f1_accuracy)/len(classifier_f1_accuracy)\n",
    "        classifier_macroAverage = sum(classifier_f1_macro)/len(classifier_f1_macro)\n",
    "        classifier_weightedAverage = sum(classifier_f1_weighted)/len(classifier_f1_weighted)\n",
    "\n",
    "        # add all averaged scores into classifier score with key being name of classifier\n",
    "        classifier_scores[classifier.__name__] = (classifier_accuracyAverage, \\\n",
    "                                                  classifier_macroAverage, classifier_weightedAverage)\n",
    "\n",
    "    return classifier_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc450bf5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
